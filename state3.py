# -*- coding: utf-8 -*-
"""
Created on Sat Sep 17 09:03:31 2016

@author: yiyuezhuo
"""

"""
当决策会带来随机而非确定的状态时，很多算法就失效了。
为了表示随机结果，我们可以具体描述随机结果然后交给算法求期望方差之类的，
这显然难以办到，特别是有连续随机变量时将是一场表示的灾难。
另一个方法是随便模拟几次，状态自己不能整合成什么代表状态的话，我们就
仅适用状态评分函数对其求简单平均作为这个决策的分数。这是利用状态评分
函数的一个方法。
对于MCT来说，我们可以做如下变化，设所使用的MCT是长期维护的。
给定MCT一个状态，它先尝试一遍所有决策n次，注意这里决策可能得到没有代表性的随机结果
然后它审查这些抽样状态在之前的MCT的VD情况，注意这里怎么整合不同信度的VD对是个复杂问题
这里不妨使用最简单的把所有胜率直接平均。然后按照某个规则（地位类似UCT函数）选择出一个
决策，再抽样得到一个状态，对这个状态再使用MCT决策，直到一轮game结束，得到胜负结果。开始
反传。所有state在1方胜或2方胜上+1，此结果在不同控制方的轮会解释成相对的VD状况。
如此，一轮迭代完成。顶层再进行这样的迭代若干次，其中顶层和后面的递归调用MCT有所不同
，顶层在反传时除了自己的1方胜和2方胜更新外，还会更新自己的各个决策的实验出的列表。
本来递归调用也可以更新这个，不过那样没有顶层更新的特殊意义，却使得复杂度大大上升，弃之。
最后输出的决策直接由类UCT函数或倾向保障胜率的UCT函数根据各个决策的实验列表输出哪个决策。
（因为UCT函数本身除了胜率外，还刻意要兼顾探索，可以使用一个只追求胜率的类UCT来优化结果，
但AI自己对弈时就没有必要了）
这样搞相当于利用VD对形成了state评分函数，我们也可以直接插入一个先验的打分函数协同决策
只要类UCT函数也综合考虑这个函数的值就可以了。
UCT不断迭代还可以产生一个state -> v1,v2 映射大表。我们可以在这个大表上跑简约模型
或者神经网络，这就是导出其他模型的基础。
"""